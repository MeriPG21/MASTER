{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e477c161-349e-453b-be46-ba82392db09a",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "376a320b672c7b44e2effcf32effc572",
     "grade": false,
     "grade_id": "cell-f8987996be9f1238",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Utilización del servicio de alquiler de bicicletas en Toronto en el año 2018\n",
    "\n",
    "### Disponible en Kaggle en el siguiente enlace (que NO debe usarse para el ejercicio, sino los CSV que se adjuntaron al email):\n",
    "https://www.kaggle.com/jackywang529/toronto-bikeshare-data\n",
    "\n",
    "\n",
    "El propósito de este análisis es utilizar los conjuntos de datos trimestrales del año 2018 de la empresa de alquiler de bicicletas en Toronto. Se trata de *cuatro* conjuntos de datos separados, que incluyen entre 178.559 y 822.536 observaciones, siempre con nueve variables. Cada fila representa un viaje realizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b97a5a-70e4-4608-82a8-ba5d8974e9ce",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a6b4dc108ddf890c659e33701965428",
     "grade": false,
     "grade_id": "cell-f74d7bfd01811789",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Variables y significado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66249de2-15e0-45f8-b773-780dcc8f2272",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cb790eed3719dc8d6cfd639c9176b4a",
     "grade": false,
     "grade_id": "cell-9cfb34982bd4eb04",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Las variables utilizadas para describir cada viaje son:\n",
    "\n",
    "* trip_id – identificador global del viaje\n",
    "* trip_duration_seconds – duración del viaje en segundos\n",
    "* from_station_id – identificador numérico de la estación de origen\n",
    "* trip_start_time – instante (timestamp) en el que se inició el viaje\n",
    "* from_station_name – nombre de la intersección más cercana a la estación origen\n",
    "* trip_stop_time – instante (timestamp) en el que finalizó el viaje\n",
    "* to_station_id – identificador numérico de la estación de destino\n",
    "* to_station_name – nombre de la intersección más cercana a la estación de destino\n",
    "* user_type – tipo de usuario (indicador binario): miembro registrado con cuota anual / usuario ocasional no registrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04fbe785-5a7a-409d-9d6e-5fd94cc8aef6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nombre completo del alumno:**  María Pérez García"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2cf0380-c5f8-41df-86a2-46c69ef74794",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7793cb7f9290ca23e841c9ede328bc84",
     "grade": false,
     "grade_id": "cell-b4f9c37a2b92d2e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# INSTRUCCIONES\n",
    "\n",
    "En cada celda debes responder a la pregunta formulada, asegurándote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del cálculo quede guardado exactamente en la variable que venía inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). \n",
    "\n",
    "**No olvides borrar la línea *raise NotImplementedError()* de cada celda cuando hayas completado la solución de esa celda y quieras probarla**.\n",
    "\n",
    "Después de cada celda evaluable verás una celda con código. Ejecútala (no modifiques su código) y te dirá si tu solución es correcta o no. En caso de ser correcta, se ejecutará correctamente y no mostrará nada, pero si no lo es mostrará un error. Además de esas pruebas, se realizarán algunas más (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la solución correcta o no. Asegúrate de que, al menos, todas las celdas indican que el código es correcto antes de enviar el notebook terminado.\n",
    "\n",
    "\n",
    "**Nunca se debe redondear ninguna cantidad si no lo pide explícitamente el enunciado**\n",
    "\n",
    "### Cada solución debe escribirse obligatoriamente en la celda habilitada para ello. Cualquier celda adicional que se haya creado durante el desarrollo deberá ser eliminada.\n",
    "\n",
    "Si necesitas crear celdas auxiliares durante el desarrollo, puedes hacerlo pero debes asegurarte de borrarlas antes de entregar el notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dfcc241-2500-4e17-9f7f-7b3fd94c1510",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bae739df33929bae8d756987e80caf8",
     "grade": false,
     "grade_id": "cell-69ec0993eeaff3ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Sobre los cuatro datasets anteriores (Bike Share Toronto Ridership_Q1 2018.csv hasta Q4) se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66823a65-6504-443b-be5c-975aaa46fe77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 1\n",
    "\n",
    "* Leer por separado cada uno de ellos (sin cachear), tratando de que Spark infiera el tipo de dato de cada columna, y **unirlos en un solo DF** que tampoco debe ser cacheada todavía, ya que en el siguiente paso aún realizaremos otro pre-procesamiento.\n",
    "* Los cuatro contienen las mismas columnas por lo que no habrá problemas para utilizar la operación `union` encadenada tres veces para crear el DF final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ce1bafd-7d2a-476e-b8d6-77e56a23b8fe",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44717972bf7cac300a5ad876d9fd6632",
     "grade": false,
     "grade_id": "read_csv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n",
    "tripsQ1 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\\\n",
    "                 .csv(\"abfss://tarea@mastermaria.dfs.core.windows.net/Bike Share Toronto Ridership_Q1 2018.csv\") \n",
    "                 # Primer CSV\n",
    "tripsQ2 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\\\n",
    "                 .csv(\"abfss://tarea@mastermaria.dfs.core.windows.net/Bike Share Toronto Ridership_Q2 2018.csv\") \n",
    "                 # Segundo CSV\n",
    "tripsQ3 =   spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\\\n",
    "                 .csv(\"abfss://tarea@mastermaria.dfs.core.windows.net/Bike Share Toronto Ridership_Q3 2018.csv\") \n",
    "                 # Tercer CSV\n",
    "tripsQ4 = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\")\\\n",
    "                 .csv(\"abfss://tarea@mastermaria.dfs.core.windows.net/Bike Share Toronto Ridership_Q4 2018.csv\")  # Cuarto CSV\n",
    "tripsTorontoRawDF = tripsQ1.union(tripsQ2).union(tripsQ3).union(tripsQ4) # Unión de todos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24982e1f-df8e-4c4b-8798-66c2ceb855ab",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e6fa646bfe97c4d7d321099133d99e4",
     "grade": true,
     "grade_id": "read_csv_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "assert(tripsTorontoRawDF.count() == 1922955)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cca1d40-7941-4ef5-a556-77f69181b54d",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6eb33253c64dbd7870725e3e6d6a8e0f",
     "grade": false,
     "grade_id": "cell-b90f5b934eda250e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 2\n",
    "\n",
    "* Las columnas `trip_start_time` y `trip_stop_time` son en realidad instantes de tiempo que Spark debería procesar como timestamp. Reemplaza **ambas columnas** por su versión convertida a timestamp, utilizando `withColumn` y donde el nuevo valor de la columna viene dado por el siguiente código:\n",
    "        F.from_unixtime(F.unix_timestamp('nombreColumna', 'M/d/yyyy H:mm')).cast(\"timestamp\"))\n",
    "El DF resultante debe ser almacenado en la variable `tripsTorontoDF`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d9284c-466f-4f6a-ac78-563ce5b0e1a7",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9129c1d06eef70a5b6922585902dfa36",
     "grade": false,
     "grade_id": "convert_timestamp",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# No olvides los imports que necesites...\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# LÍNEAS EVALUABLES, NO RENOMBRAR LAS VARIABLES\n",
    "tripsTorontoDF = tripsTorontoRawDF \\\n",
    "    .withColumn(\"trip_start_time\", F.from_unixtime(F.unix_timestamp('trip_start_time', 'M/d/yyyy H:mm')).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"trip_stop_time\", F.from_unixtime(F.unix_timestamp('trip_stop_time', 'M/d/yyyy H:mm')).cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9d2fa3-398f-47b5-bc10-962b7fa8b1bf",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ebd685fd8c8fcd5062ecd1c29adcd4b",
     "grade": true,
     "grade_id": "convert_timestamp_tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "typesDict = dict(tripsTorontoDF.dtypes)\n",
    "assert(typesDict[\"trip_start_time\"] == \"timestamp\") \n",
    "assert(typesDict[\"trip_stop_time\"] == \"timestamp\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94bb01f5-e30f-4e38-91aa-9df301e46451",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d11d72889323abc3fa6626ed1da257f",
     "grade": false,
     "grade_id": "cell-fc88821f19453a51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 3\n",
    "\n",
    "Partiendo de `tripsTorontoDF`, realizar las siguientes transformaciones encadenadas en este orden para crear un nuevo DF:\n",
    "* Primero, debemos quedarnos solamente con las filas donde `trip_start_time` no sea null.\n",
    "* Sobre el DF resultado de lo anterior, añadir una columna adicional **Mes** y con el mes representado en **trip_start_time**. Dicha columna será de tipo entero y se puede obtener usando `withColumn` con la función `F.month(\"colName\")`, que recibe un nombre de columna y devuelve un objeto columna de enteros que van de 1 a 12. \n",
    "* Encadenar esta transformación con otra en la que la columna **Mes** sea reemplazada por su traducción a  cadena de caracteres de 3 letras, siendo la correspondencia 1: Ene, 2: Feb, 3: Mar, 4: Abr, 5: May, 6: Jun, 7: Jul, 8: Ago, 9: Sep, 10: Oct, 11: Nov, 12: Dic.\n",
    "* Finalmente, añadir una nueva columna **Hora** que contenga la hora de inicio del viaje, aplicando `withColumn` con la función `F.hour(\"colName\")` que recibe un nombre de columna y recibe un objeto columna de enteros de 0 a 23.\n",
    "* El DF resultante de todas estas transformaciones debe guardarse en la variable `tripsTorontoTimesDF`, que por tanto tendrá 2 columnas más que el DF original `tripsTorontoDF`, y que debe quedar **cacheado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a61347-c799-4303-b828-de82fc2efd2f",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aa969fc933e984b995caf9a857feede",
     "grade": false,
     "grade_id": "renombrar_mes_hora",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "tripsTorontoTimesDF = tripsTorontoDF.filter(F.col(\"trip_start_time\").isNotNull()) \\\n",
    "    .withColumn(\"Mes\", F.month(\"trip_start_time\")) \\\n",
    "    .withColumn(\"Mes\", F.when(F.col(\"Mes\") == 1, \"Ene\")\n",
    "                      .when(F.col(\"Mes\") == 2, \"Feb\")\n",
    "                      .when(F.col(\"Mes\") == 3, \"Mar\")\n",
    "                      .when(F.col(\"Mes\") == 4, \"Abr\")\n",
    "                      .when(F.col(\"Mes\") == 5, \"May\")\n",
    "                      .when(F.col(\"Mes\") == 6, \"Jun\")\n",
    "                      .when(F.col(\"Mes\") == 7, \"Jul\")\n",
    "                      .when(F.col(\"Mes\") == 8, \"Ago\")\n",
    "                      .when(F.col(\"Mes\") == 9, \"Sep\")\n",
    "                      .when(F.col(\"Mes\") == 10, \"Oct\")\n",
    "                      .when(F.col(\"Mes\") == 11, \"Nov\")\n",
    "                      .otherwise(\"Dic\")) \\\n",
    "    .withColumn(\"Hora\", F.hour(\"trip_start_time\")) \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f393b53-fd23-43b6-9efb-46eadb501dc5",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b60aed54e852d77c7fa8482a4c34379c",
     "grade": true,
     "grade_id": "renombrar_mes_hora_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tripsPerMonth = tripsTorontoTimesDF.groupBy(\"Mes\").count().sort(\"Mes\").collect()\n",
    "assert(tripsPerMonth[0][\"count\"] == 94783)\n",
    "assert(tripsPerMonth[1][\"count\"] == 281219)\n",
    "assert(tripsPerMonth[2][\"count\"] == 83324)\n",
    "assert(tripsPerMonth[3][\"count\"] == 43859)\n",
    "assert(tripsPerMonth[4][\"count\"] == 49731)\n",
    "assert(tripsPerMonth[5][\"count\"] == 286316)\n",
    "assert(tripsPerMonth[6][\"count\"] == 250837)\n",
    "assert((tripsPerMonth[7][\"count\"] == 84959) | (tripsPerMonth[7][\"count\"] == 84969))\n",
    "assert(tripsPerMonth[8][\"count\"] == 212750)\n",
    "assert(tripsPerMonth[9][\"count\"] == 104287)\n",
    "assert(tripsPerMonth[10][\"count\"] == 175879)\n",
    "assert(tripsPerMonth[11][\"count\"] == 255001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d73fef0-06ec-41e4-9fb8-669eb6b1f800",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f98c8387ad8a683f0ea2f1c6e441e07a",
     "grade": false,
     "grade_id": "cell-a71a6b17b1e0d613",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 4\n",
    "\n",
    "* Partiendo de `tripsTorontoTimesDF`, crear un nuevo DataFrame con **tantas filas como horas tiene el día, y tantas columnas como meses del año** de manera que cada celda indique el **número de viajes** que comenzaron a esa hora en ese mes del año. Guardar el resultado en la variable `tripsPerMonthAndHourDF`, cuyas filas deben quedar ordenadas en base a la hora (de 0 a 23), y cuyas columnas deben estar también ordenadas desde `\"Ene\"` a `\"Dic\"`, con `\"Hora\"` como primera columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1cc8b06-0df4-462b-900c-9171c4d13a82",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "416095b927a3c16ca9843df1228e43d3",
     "grade": false,
     "grade_id": "numero_categorias",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "ordered_months = [\"Ene\", \"Feb\", \"Mar\", \"Abr\", \"May\", \"Jun\", \"Jul\", \"Ago\", \"Sep\", \"Oct\", \"Nov\", \"Dic\"]\n",
    "\n",
    "tripsPerMonthAndHourDF = tripsTorontoTimesDF.groupBy(\"Hora\") \\\n",
    "    .pivot(\"Mes\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"Hora\") \\\n",
    "    .select([\"Hora\"] + [col(month) for month in ordered_months])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e3a61a-14ce-461c-8ad2-635e69ce4a67",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d64ff89bcbd46871e937ae34db834496",
     "grade": true,
     "grade_id": "numero_categorias_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(len(tripsPerMonthAndHourDF.columns) == 13)\n",
    "assert(tripsPerMonthAndHourDF.columns[0] == \"Hora\")\n",
    "assert(tripsPerMonthAndHourDF.columns[12] == \"Dic\")\n",
    "assert(tripsPerMonthAndHourDF.count() == 24)\n",
    "todasHoras = tripsPerMonthAndHourDF.collect()\n",
    "assert((todasHoras[0][\"Hora\"] == 0) & (todasHoras[0][\"Dic\"]==782))\n",
    "assert((todasHoras[23][\"Hora\"] == 23) & (todasHoras[23][\"Dic\"]==1208))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87aa66f0-2758-426d-82f4-f2be0b3e2389",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d2b0a3fb505c65793ee15bf17e87e87",
     "grade": false,
     "grade_id": "cell-c5ec05706eccd480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(3 puntos)** Ejercicio 5. \n",
    "\n",
    "Partiendo de `tripsTorontoTimesDF` definido anteriormente, añadir las siguientes columnas:\n",
    "\n",
    "* Primero, tres columnas adicionales llamadas `dur_media`, `dur_min`, `dur_max` que contengan, respectivamente, **la duración media, mínima y máxima de los viajes que parten de esa misma estación de origen (from_station_id) a esa misma hora y en ese mismo mes del año**. Es decir, queremos una columna extra para que podamos tener, junto a cada viaje, información agregada de los viajes similares, entendidos como aquellos que salieron a la misma hora de la misma estación. **No se debe utilizar JOIN sino solo funciones de ventana**.\n",
    "* A continuación, otra columna adicional `diff_dur_porc` que contenga la diferencia, medida en porcentaje, entre la duración del viaje y la duración media de los viajes similares calculada en el apartado anterior. Dicha diferencia debe calcularse como la resta de la duración del viaje menos la duración media, dividida entre la duración media y multiplicada por 100. El resultado debe obtenerse aplicando operaciones aritméticas con columnas existentes, **sin utilizar `when`**.\n",
    "* El DF resultante con las 4 columnas nuevas que hemos añadido debe almacenarse en la variable `tripsTorontoExtraInfoDF`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86df7b8f-2490-4aeb-8143-1656d50c67a4",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14773754aebd257287c3ab4a00b00379",
     "grade": false,
     "grade_id": "ventana",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "# imports necesarios..........\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "windowHoraMesEstacion = Window.partitionBy(\"Hora\",\"Mes\",\"from_station_id\")\n",
    "\n",
    "tripsTorontoExtraInfoDF = tripsTorontoTimesDF \\\n",
    "    .withColumn(\"dur_media\", F.avg(\"trip_duration_seconds\").over(windowHoraMesEstacion).cast(\"double\")) \\\n",
    "    .withColumn(\"dur_min\", F.min(\"trip_duration_seconds\").over(windowHoraMesEstacion).cast(\"double\")) \\\n",
    "    .withColumn(\"dur_max\", F.max(\"trip_duration_seconds\").over(windowHoraMesEstacion).cast(\"double\")) \\\n",
    "    .withColumn(\"diff_dur_porc\", ((F.col(\"trip_duration_seconds\") - F.col(\"dur_media\")) / F.col(\"dur_media\")) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e9d7d29-3d90-45d6-807c-707507687617",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "729b30b98cb70cb301206a9bce962a58",
     "grade": true,
     "grade_id": "ventana_test",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "r = tripsTorontoExtraInfoDF.where(\"trip_id = '2970611'\").head()\n",
    "assert(r.dur_media - 783.366666667 < 0.001)\n",
    "assert(r.diff_dur_porc - 44.24918088591975 < 0.001)\n",
    "assert(r.dur_min == 167)\n",
    "assert(r.dur_max == 2333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0276f73-09d7-4bb0-9b70-96c2c7d58e23",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d49cdc97f3973ce93b2cfb164310714",
     "grade": false,
     "grade_id": "cell-9ebe35c4b4325269",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(3 puntos)** Ejercicio 6\n",
    "\n",
    "* Partiendo de `tripsTorontoTimesDF`, crear un **grafo** llamado `bikeGraph` utilizando como identificador de los vértices los identificadores de las estaciones. Construye primero un DF con todos los identificadores de las estaciones, simplemente seleccionando **from_station_id**, renombrando adecuadamente el nombre de columna. Puedes almacenar este DF en la variable `verticesDF`. También tendrás que renombrar las columnas **from_station_id** y **to_station_id** en el DF de aristas, para el que además deberás seleccionar solo dichas columnas y quitar las filas repetidas ya que solo necesitamos considerar una vez cada ruta (cada pareja de estación inicial y final). Puedes almacenar el resultado del renombramiento y la eliminación de repetidos en la variable `edgesDF`.\n",
    "* Una vez creado, aplica el algoritmo `pageRank` pasando como **ÚNICO** parámetro `maxIter = 5`, y **ningún parámetro más**. El algoritmo puede llegar a emplear más de 10 minutos. \n",
    "* Almacena el grafo devuelto por dicha función en la variable `pageRankGraph`, recupera el DF de sus vértices, ordénalo descendentemente en base a la columna `pagerank` y almacena el resultado en la variable `sortedPageRankGraphVerticesDF`\n",
    "* Obtén el identificador de la estación más relevante (con mayor valor de la métrica pageRank, que ocupará la primera fila tras la ordenación), y almacena dicho identificador en la variable `id_mas_relevante`.\n",
    "* Crea un nuevo DF de una sola fila y tres columnas llamadas `dur_media`, `dur_min` y `dur_max` con la duración **media, mínima y máxima** de los viajes de `tripsTorontoTimesDF` que **empiezan** en dicha estación (sin tener en cuenta distinción de horas o meses). **No debe usarse la función `withColumn` sino crear las columnas al vuelo con `select`**. Debe quedar almacenado en la variable `durEstMasRelevantesDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7664876-0b60-429e-adb5-e0de3be2cb08",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc1f29ab4015788d03e8e01be66ea200",
     "grade": false,
     "grade_id": "graph",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE, NO RENOMBRAR VARIABLES\n",
    "# imports necesarios..........\n",
    "from pyspark.sql.functions import col\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# Descomentar la siguiente línea antes de lanzar pageRank:\n",
    "\n",
    "verticesDF = tripsTorontoTimesDF.select(F.col(\"from_station_id\").alias(\"id\")).distinct().cache()\n",
    "\n",
    "edgesDF = tripsTorontoTimesDF.withColumnRenamed(\"from_station_id\", \"src\")\\\n",
    "                   .withColumnRenamed(\"to_station_id\", \"dst\")\\\n",
    "                   .select(\"src\", \"dst\")\\\n",
    "                   .distinct()\\\n",
    "                   .cache()\n",
    "\n",
    "bikeGraph = GraphFrame(verticesDF, edgesDF)\n",
    "\n",
    "pageRankGraph = bikeGraph.pageRank(maxIter=5)\n",
    "\n",
    "sortedPageRankGraphVerticesDF = pageRankGraph.vertices \\\n",
    "    .orderBy(col(\"pagerank\").desc()) \\\n",
    "    .withColumn(\"id\", col(\"id\").cast(\"double\"))\n",
    "\n",
    "id_mas_relevante = sortedPageRankGraphVerticesDF.select(\"id\").first()[0]\n",
    "\n",
    "durEstMasRelevantesDF = tripsTorontoTimesDF.filter(col(\"from_station_id\") == id_mas_relevante) \\\n",
    "    .selectExpr(\"avg(trip_duration_seconds) AS dur_media\", \"min(trip_duration_seconds) AS dur_min\", \"max(trip_duration_seconds) AS dur_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50fc650c-70ef-4006-aeff-d92b60934fde",
     "showTitle": false,
     "title": ""
    },
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cce36154f8b55fd019b7f285e1273958",
     "grade": true,
     "grade_id": "graph_tests",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(sortedPageRankGraphVerticesDF.head()[\"pagerank\"] - 1.4427 < 0.01)\n",
    "assert(durEstMasRelevantesDF.count() == 1)\n",
    "assert(len(durEstMasRelevantesDF.columns) == 3)\n",
    "rEstMasRelevantes = durEstMasRelevantesDF.head()\n",
    "assert(rEstMasRelevantes.dur_min == 61)\n",
    "assert(id_mas_relevante == 7060)\n",
    "assert(rEstMasRelevantes.dur_media - 747.6957692082626 < 0.001)\n",
    "assert(rEstMasRelevantes.dur_max == 35130)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark_m3",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
